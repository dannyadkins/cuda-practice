This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-11T20:10:07.134Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
algos/
  matmul/
    kernel.cu
    main.py
  newton_schulz/
    baseline.py
    newton_schulz_binding.cpp
    newton_schulz_kernel.cu
cv/
  rgb_to_grayscale/
    kernel.cu
    main.py
utils/
  compile_extension.py
.gitignore
setup.py

================================================================
Repository Files
================================================================

================
File: algos/matmul/kernel.cu
================
#include <c10/cuda/CUDAException.h>
#include <c10/cuda/CUDAStream.h>
#include <iostream>

torch::Tensor matmul(torch::Tensor a, torch::Tensor b) {
    std::cout << "Tensor a is is on: " << a.device() 
    assert(a.device().type() == torch::kCUDA);
    assert(b.device().type() == torch::kCUDA);
    assert(a.dtype() == torch::kByte);
    assert(b.dtype() == torch::kByte);

    // ensure that matmul sizes are appropriate
    const auto n_rows_A = a.size(0);
    const auto n_rows_B = b.size(0);

    const auto n_cols_A = a.size(1);
    const auto n_cols_B = b.size(1);

    assert(n_cols_A == n_cols_B);
    
}

================
File: algos/matmul/main.py
================
import sys
from pathlib import Path
import torch
from torchvision.io import read_image, write_png
from torch.utils.cpp_extension import load_inline

def compile_extension(file_name, cpp_fn_signature, cpp_fn_name):
    if (Path(file_name).suffix != ".cu"):
        raise Exception("File must be a .cu file")

    cuda_source = Path(file_name).read_text()

    # Load the CUDA kernel as a PyTorch extension
    ext = load_inline(
        name=file_name.split(".")[0],
        cpp_sources=cpp_fn_signature,
        cuda_sources=cuda_source,
        functions=[cpp_fn_name],
        with_cuda=True,
        extra_cuda_cflags=["-O2"],
        # build_directory='./cuda_build',
    )
    return ext

def main():
    A = torch.randn((10,20)).cuda()
    B = torch.randn((20,30)).cuda()

    print("A.device: ", A.device)

    ext = compile_extension("kernel.cu", "torch::Tensor matmul(torch::Tensor a, torch::Tensor b);", "matmul")
    
    ext.matmul(A,B)

if __name__ == "__main__":
    main()

================
File: algos/newton_schulz/baseline.py
================
import torch

@torch.compile
def zeropower_via_newtonschulz5_baseline(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    if G.size(0) > G.size(1):
        X = X.T
    return X


zeropower_backends = {
    'svd': zeropower_via_newtonschulz5_baseline,
    'newtonschulz5': zeropower_via_newtonschulz5_cuda
}

================
File: algos/newton_schulz/newton_schulz_binding.cpp
================
// newton_schulz_binding.cpp

#include <torch/extension.h>

void newton_schulz_cuda(
    torch::Tensor G,
    torch::Tensor X,
    int steps,
    float a,
    float b,
    float c) {
    const auto N = G.size(0);
    const auto M = G.size(1);

    // Allocate buffer for A and B
    auto buffer = torch::empty({2 * N * N}, G.options());

    // Determine block and grid sizes
    const int threads = 16;
    const dim3 block_dim(threads, threads);
    const dim3 grid_dim((M + threads - 1) / threads, (N + threads - 1) / threads);

    AT_DISPATCH_FLOATING_TYPES_AND_HALF(G.scalar_type(), "newton_schulz_cuda", ([&] {
        newton_schulz_kernel<scalar_t><<<grid_dim, block_dim>>>(
            G.data_ptr<scalar_t>(),
            X.data_ptr<scalar_t>(),
            buffer.data_ptr<scalar_t>(),
            N,
            M,
            steps,
            static_cast<scalar_t>(a),
            static_cast<scalar_t>(b),
            static_cast<scalar_t>(c)
        );
    }));

    // Check for CUDA errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess)
        throw std::runtime_error(cudaGetErrorString(err));
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("newton_schulz_cuda", &newton_schulz_cuda, "Newton-Schulz CUDA kernel");
}

================
File: algos/newton_schulz/newton_schulz_kernel.cu
================
// newton_schulz_kernel.cu

#include <torch/extension.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda.h>
#include <cuda_runtime.h>

template <typename scalar_t>
__global__ void newton_schulz_kernel(
    const scalar_t* __restrict__ G,
    scalar_t* __restrict__ X,
    scalar_t* __restrict__ buffer,
    const int N,
    const int M,
    const int steps,
    const scalar_t a,
    const scalar_t b,
    const scalar_t c) {
    // Shared memory for tiles (optional)
    // extern __shared__ scalar_t shared_data[];

    // Compute global thread indices
    int row = blockIdx.y * blockDim.y + threadIdx.y; // row index
    int col = blockIdx.x * blockDim.x + threadIdx.x; // column index

    // Guard against out-of-bounds threads
    if (row >= N || col >= M)
        return;

    // Load G into X as initial value
    X[row * M + col] = G[row * M + col];

    // Initialize buffer pointers
    scalar_t* A = buffer;            // Size N*N
    scalar_t* B = buffer + N * N;    // Size N*N

    // Synchronize to ensure all threads have loaded initial X
    __syncthreads();

    // Perform iterations
    for (int step = 0; step < steps; ++step) {
        // Compute A = X @ X^T
        scalar_t sum_A = 0;
        for (int k = 0; k < M; ++k) {
            scalar_t x_ik = X[row * M + k];
            scalar_t x_jk = X[col * M + k]; // Note: Transposed
            sum_A += x_ik * x_jk;
        }
        // Store A[row, col]
        if (row < N && col < N) {
            A[row * N + col] = sum_A;
        }

        __syncthreads();

        // Compute B = b * A + c * A @ A
        scalar_t sum_B = 0;
        if (row < N && col < N) {
            // Compute (A @ A)[row, col]
            scalar_t sum_AA = 0;
            for (int k = 0; k < N; ++k) {
                sum_AA += A[row * N + k] * A[k * N + col];
            }
            // Compute B[row, col]
            B[row * N + col] = b * A[row * N + col] + c * sum_AA;
        }

        __syncthreads();

        // Update X = a * X + B @ X
        scalar_t sum_X = 0;
        for (int k = 0; k < N; ++k) {
            scalar_t b_ik = B[row * N + k];
            scalar_t x_kj = X[k * M + col];
            sum_X += b_ik * x_kj;
        }
        // Update X[row, col]
        X[row * M + col] = a * X[row * M + col] + sum_X;

        __syncthreads();
    }
}

================
File: cv/rgb_to_grayscale/kernel.cu
================
#include <c10/cuda/CUDAException.h>
#include <c10/cuda/CUDAStream.h>


__global__
void rgb_to_grayscale_kernel(unsigned char* output, unsigned char* input, int width, int height) {
    const int channels = 3;

    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    if (col < width && row < height) {
        int outputOffset = row * width + col;
        int inputOffset = (row * width + col) * channels;

        unsigned char r = input[inputOffset + 0];   // red
        unsigned char g = input[inputOffset + 1];   // green
        unsigned char b = input[inputOffset + 2];   // blue

        output[outputOffset] = (unsigned char)(0.21f * r + 0.71f * g + 0.07f * b);
    }
}


// helper function for ceiling unsigned integer division
inline unsigned int cdiv(unsigned int a, unsigned int b) {
  return (a + b - 1) / b;
}


torch::Tensor rgb_to_grayscale(torch::Tensor image) {
    assert(image.device().type() == torch::kCUDA);
    assert(image.dtype() == torch::kByte);

    const auto height = image.size(0);
    const auto width = image.size(1);

    auto result = torch::empty({height, width, 1}, torch::TensorOptions().dtype(torch::kByte).device(image.device()));

    dim3 threads_per_block(16, 16);     // using 256 threads per block
    dim3 number_of_blocks(cdiv(width, threads_per_block.x),
                          cdiv(height, threads_per_block.y));

    rgb_to_grayscale_kernel<<<number_of_blocks, threads_per_block, 0, torch::cuda::getCurrentCUDAStream()>>>(
        result.data_ptr<unsigned char>(),
        image.data_ptr<unsigned char>(),
        width,
        height
    );

    // check CUDA error status (calls cudaGetLastError())
    C10_CUDA_KERNEL_LAUNCH_CHECK();

    return result;
}

================
File: cv/rgb_to_grayscale/main.py
================
import sys
from pathlib import Path

import torch
from torchvision.io import read_image, write_png
from torch.utils.cpp_extension import load_inline
# from utils.compile_extension import compile_extension

def compile_extension(file_name, cpp_fn_signature, cpp_fn_name):
    if (Path(file_name).suffix != ".cu"):
        raise Exception("File must be a .cu file")

    cuda_source = Path(file_name).read_text()

    # Load the CUDA kernel as a PyTorch extension
    ext = load_inline(
        name=file_name.split(".")[0],
        cpp_sources=cpp_fn_signature,
        cuda_sources=cuda_source,
        functions=[cpp_fn_name],
        with_cuda=True,
        extra_cuda_cflags=["-O2"],
        # build_directory='./cuda_build',
    )
    return ext


def main():
    """
    Use torch cpp inline extension function to compile the kernel in grayscale_kernel.cu.
    Read input image, convert it to grayscale via custom cuda kernel and write it out as png.
    """
    ext = compile_extension(file_name="kernel.cu", cpp_fn_signature="torch::Tensor rgb_to_grayscale(torch::Tensor input);", cpp_fn_name="rgb_to_grayscale")

    x = read_image("img.jpg").permute(1, 2, 0).cuda()
    print("mean:", x.float().mean())
    print("Input image:", x.shape, x.dtype)

    assert x.dtype == torch.uint8

    y = ext.rgb_to_grayscale(x)

    print("Output image:", y.shape, y.dtype)
    print("mean", y.float().mean())
    write_png(y.permute(2, 0, 1).cpu(), "output.png")


if __name__ == "__main__":
    main()

================
File: utils/compile_extension.py
================
from pathlib import Path
import torch
from torchvision.io import read_image, write_png
from torch.utils.cpp_extension import load_inline


def compile_extension(file_name, cpp_fn_signature, cpp_fn_name):
    if (Path(file_name).suffix != ".cu"):
        raise Exception("File must be a .cu file")

    cuda_source = Path(file_name).read_text()

    # Load the CUDA kernel as a PyTorch extension
    ext = load_inline(
        name=file_name.split(".")[0],
        cpp_sources=cpp_fn_signature,
        cuda_sources=cuda_source,
        functions=[cpp_fn_name],
        with_cuda=True,
        extra_cuda_cflags=["-O2"],
        # build_directory='./cuda_build',
    )
    return ext

================
File: .gitignore
================
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# PEP 582; used by e.g. github.com/David-OConnor/pyflow
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# cuda
*.o
*.ptx
*.cubin
*.fatbin
*.fatbin.c
*.hash
*.cudafe*
*.cudafe1*
*.cudafe2*
*.cu.cpp.ii
*.cu.cpp
*.ii
*.nvcc_out
*.module_id
*.reg.c
*.s
*.asm
*.cu.c
*.cu.cpp
*.ptx.c
*.cubin.c
*.fatbin.c
*.hash.c
*.cudafe*.c
*.cudafe1*.c
*.cudafe2*.c
*.cu.cpp.ii.c
*.cu.cpp.c
*.ii.c
*.nvcc_out.c
*.module_id.c
*.reg.c.c
*.s.c
*.asm.c

# torch
*.pth

================
File: setup.py
================
from setuptools import setup, find_packages
from torch.utils.cpp_extension import BuildExtension, CUDAExtension
from pathlib import Path

# Find all CUDA source files
cuda_sources = []
for cuda_file in Path('algos').rglob('*.cu'):
    cuda_sources.append(str(cuda_file))

for cuda_file in Path('algos').rglob('*_binding.cpp'): 
    cuda_sources.append(str(cuda_file))

setup(
    name='cuda-practice',
    version='0.1',
    packages=find_packages(),
    ext_modules=[
        CUDAExtension(
            name='cuda_kernels',
            sources=cuda_sources,
            extra_compile_args={'cxx': ['-O2'],
                              'nvcc': ['-O2']}
        ),
    ],
    cmdclass={
        'build_ext': BuildExtension
    }
)
